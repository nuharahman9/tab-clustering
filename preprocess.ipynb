{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load libraries / create tokenizer object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import string \n",
    "import numpy as np \n",
    "import os \n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords   \n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF \n",
    "from collections import Counter \n",
    "import math\n",
    "import re "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train / test split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text\n",
      "0       Man continuously undergoes selection through t...\n",
      "1       Today we are celebrating the precious letter P...\n",
      "2       Comics Archive | I made inky. 07/30/12 â€“ Comic...\n",
      "3       This article is about current Disney record la...\n",
      "4       This entry was posted on November 3, 2011 at 4...\n",
      "...                                                   ...\n",
      "149995  17 HOU W 22-13 2 0 0 0.0 0 No injury listed. 1...\n",
      "149996  Item description Caption on back of photograph...\n",
      "149997  In a bizarre coincidence recently, DC Comics l...\n",
      "149998  The Men of Hampton Park dedicate themselves to...\n",
      "149999  Welcome to Post-weekend Poetry and the twelfth...\n",
      "\n",
      "[150000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('c4filteredsample_tagged.csv')\n",
    "\n",
    "for col in data.columns: \n",
    "    if \"text\" not in col: \n",
    "        data = data.drop(col, axis=1)\n",
    "\n",
    "\n",
    "data = data.replace({r'\\n' : ' '}, regex=True) \n",
    "data.fillna('', inplace=True)\n",
    "data = data.replace({r'\\'' : ' '}, regex=True) \n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./corpus/www.aa.com_booking_find-flights.txt\n",
      "./corpus/www.linkedin.com_jobs_search_?currentJobId=3915603480&distance=25&f_WT=1%2C3%2C2&geoId=102448103&keywords=software%20engineer&origin=JOBS_HOME_SEARCH_CARDS.txt\n",
      "./corpus/spacelift.io_blog_kubernetes-cronjob.txt\n",
      "./corpus/stackoverflow.com_questions_45019753_how-to-retrain-a-machine-learning-model-in-python-till-we-get-desired-outcome.txt\n",
      "./corpus/towardsdatascience.com_topic-modeling-articles-with-nmf-8c6b2a227a45.txt\n",
      "./corpus/www.frontiersin.org_articles_10.3389_fsoc.2022.886498_full.txt\n",
      "./corpus/www.jetblue.com_flights.txt\n",
      "./corpus/kubernetes.io_docs_concepts_workloads_controllers_cron-jobs_.txt\n",
      "./corpus/kubernetes.io_docs_tasks_job_automated-tasks-with-cron-jobs_.txt\n"
     ]
    }
   ],
   "source": [
    "def load_corpus(path): \n",
    "    corpus_paths = []\n",
    "    for filename in os.listdir(path): \n",
    "        file_path = path + filename \n",
    "        corpus_paths.append(file_path)\n",
    "    return corpus_paths \n",
    "\n",
    "corpus_dir = './corpus/'\n",
    "file_paths = load_corpus(corpus_dir)\n",
    "for path in file_paths: \n",
    "    print(path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "def read_txt(file_path): \n",
    "    # read file contents by line \n",
    "    file = open(file_path, \"r\", encoding=\"utf-8\")\n",
    "    text = file.read() \n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    tokenized_words = word_tokenize(text)\n",
    "    tokenized_words = [w for w in tokenized_words if w not in stop]\n",
    "    tokenized_words = [ps.stem(word) for word in tokenized_words]\n",
    "    return ' '.join(tokenized_words)\n",
    "\n",
    "\n",
    "\n",
    "websites_preprocessed_data = [] \n",
    "for path in file_paths: \n",
    "    websites_preprocessed_data.append(read_txt(path))\n",
    "\n",
    "\n",
    "print(websites_preprocessed_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF / IDF Frequency (from scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_occurrences = {}\n",
    "def get_frequencies(website): # takes the corpus, counts its frequencies occurred in doc and stores in dictionary \n",
    "    word_freq = {} # stores the frequency of each word in a doc \n",
    "    for word in website: \n",
    "        if word in word_freq: \n",
    "            word_freq[word] += 1 \n",
    "        else: \n",
    "            word_freq[word] = 1 \n",
    "    return word_freq \n",
    "\n",
    "def get_doc_occurrences(website_dict, websites, doc_occurrences): # get the number of websites a word appears in \n",
    "    for key in website_dict: \n",
    "        if key not in doc_occurrences: \n",
    "            ct = 0 \n",
    "            for website in websites: \n",
    "                if key in website: \n",
    "                    ct = ct + 1 \n",
    "            doc_occurrences[key] = ct \n",
    "    return doc_occurrences\n",
    "\n",
    "\n",
    "def calculate_tf_idf(website, websites, n, doc_occurrences): \n",
    "    word_freqs = get_frequencies(website)\n",
    "    tf_idf = {}\n",
    "    doc_occurrences = get_doc_occurrences(word_freqs, websites, doc_occurrences)\n",
    "    tf_idf = {} \n",
    "    for key in word_freqs: \n",
    "        doc_occurs = doc_occurrences[key]\n",
    "        print(doc_occurs, word_freqs[key])\n",
    "        tf_idf[key] = word_freqs[key] * math.log(n / doc_occurs)\n",
    "    print(tf_idf)\n",
    "    return tf_idf \n",
    "\n",
    "\n",
    "for website in websites_preprocessed_data: \n",
    "    calculate_tf_idf(website, websites_preprocessed_data, len(websites_preprocessed_data), doc_occurrences=doc_occurrences)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# actual TF / IDF Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 87)\t0.0312569026559189\n",
      "  (0, 12371)\t0.0312569026559189\n",
      "  (0, 18643)\t0.0312569026559189\n",
      "  (0, 22755)\t0.0312569026559189\n",
      "  (0, 13457)\t0.0312569026559189\n",
      "  (0, 14100)\t0.0312569026559189\n",
      "  (0, 11531)\t0.0312569026559189\n",
      "  (0, 9839)\t0.0312569026559189\n",
      "  (0, 8729)\t0.0312569026559189\n",
      "  (0, 101)\t0.0312569026559189\n",
      "  (0, 12378)\t0.0312569026559189\n",
      "  (0, 2693)\t0.0312569026559189\n",
      "  (0, 15597)\t0.0312569026559189\n",
      "  (0, 129)\t0.0312569026559189\n",
      "  (0, 3676)\t0.0312569026559189\n",
      "  (0, 21180)\t0.0312569026559189\n",
      "  (0, 9240)\t0.0312569026559189\n",
      "  (0, 8719)\t0.0312569026559189\n",
      "  (0, 91)\t0.0312569026559189\n",
      "  (0, 12373)\t0.0312569026559189\n",
      "  (0, 8586)\t0.0312569026559189\n",
      "  (0, 18740)\t0.0312569026559189\n",
      "  (0, 4490)\t0.0312569026559189\n",
      "  (0, 15382)\t0.0312569026559189\n",
      "  (0, 8725)\t0.0312569026559189\n",
      "  :\t:\n",
      "  (8, 4137)\t0.013793040708072423\n",
      "  (8, 18958)\t0.013793040708072423\n",
      "  (8, 20066)\t0.013793040708072423\n",
      "  (8, 18970)\t0.013793040708072423\n",
      "  (8, 10477)\t0.013793040708072423\n",
      "  (8, 17515)\t0.013793040708072423\n",
      "  (8, 8855)\t0.013793040708072423\n",
      "  (8, 5043)\t0.013793040708072423\n",
      "  (8, 2052)\t0.013793040708072423\n",
      "  (8, 10876)\t0.027586081416144846\n",
      "  (8, 8838)\t0.027586081416144846\n",
      "  (8, 17274)\t0.027586081416144846\n",
      "  (8, 21944)\t0.041379122124217266\n",
      "  (8, 10358)\t0.013793040708072423\n",
      "  (8, 3240)\t0.013793040708072423\n",
      "  (8, 4357)\t0.027586081416144846\n",
      "  (8, 10902)\t0.01586371321632449\n",
      "  (8, 17538)\t0.013793040708072423\n",
      "  (8, 10402)\t0.041379122124217266\n",
      "  (8, 4834)\t0.11104599251427145\n",
      "  (8, 4670)\t0.01586371321632449\n",
      "  (8, 1776)\t0.06896520354036212\n",
      "  (8, 4926)\t0.013793040708072423\n",
      "  (8, 5214)\t0.013793040708072423\n",
      "  (8, 198)\t0.01586371321632449\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(norm='l2', smooth_idf=True, ngram_range=(2, 3), stop_words='english')\n",
    "website_content = [] \n",
    "def file_to_st(file_path): \n",
    "    file = open(file_path, \"r\", encoding=\"utf-8\")\n",
    "    text = file.read() \n",
    "    return text \n",
    "\n",
    " \n",
    "x = vectorizer.fit_transform(websites_preprocessed_data)\n",
    "tokens = vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(data = x.T.toarray(),index = tokens, columns = file_paths)\n",
    "\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37997, 9)\n",
      "(37997, 9)\n",
      "w:  [[1.04316829e-05 4.75269088e-04 1.37509973e-02]\n",
      " [1.04316829e-05 4.75269088e-04 1.37509973e-02]\n",
      " [1.04316830e-05 4.75269088e-04 1.37509973e-02]\n",
      " ...\n",
      " [4.36836389e-06 3.11687796e-03 0.00000000e+00]\n",
      " [4.36836389e-06 3.11687796e-03 0.00000000e+00]\n",
      " [4.36836389e-06 3.11687796e-03 0.00000000e+00]] (37997, 3)\n",
      "h:  [[5.38055983e-04 5.40236319e-04 1.42902535e-01 2.59402116e-04\n",
      "  3.23789135e-04 1.20511361e-03 3.96638816e-06 7.96448511e-01\n",
      "  7.90426762e-01]\n",
      " [1.94221178e-02 1.10095831e-02 5.34009730e-03 4.76365390e-01\n",
      "  5.67451136e-01 6.81260031e-01 1.36646055e-02 0.00000000e+00\n",
      "  0.00000000e+00]\n",
      " [5.55184223e-01 4.64096898e-01 1.46886400e-02 8.76632064e-03\n",
      "  0.00000000e+00 0.00000000e+00 6.89797225e-01 0.00000000e+00\n",
      "  0.00000000e+00]] (3, 9)\n"
     ]
    }
   ],
   "source": [
    "# rank < min(m, n) for matrix m x n \n",
    "def svd_init(m, rank): \n",
    "    # convert from dataframe to array \n",
    "    arr = m.values\n",
    "    u, s, v = np.linalg.svd(arr, full_matrices=False)\n",
    "    v = v.T \n",
    "    w = np.zeros((arr.shape[0], rank)) \n",
    "    h = np.zeros((rank, arr.shape[1]))\n",
    "\n",
    "    w[:, 0] = np.sqrt(s[0]) * np.abs(u[:, 0])\n",
    "    h[0, :] = np.sqrt(s[0]) * np.abs(v[:, 0].T)\n",
    "\n",
    "    for i in range(1, rank): \n",
    "        print(u.shape)\n",
    "        x = u[:, i]\n",
    "        y = v[:, i]\n",
    "        x_p = (x >= 0) * x\n",
    "        x_n = (x < 0) * -x\n",
    "        y_p = (y >= 0) * y\n",
    "        y_n = (y <  0) * -y\n",
    "\n",
    "        xp_norm = np.linalg.norm(x_p, 2)\n",
    "        xn_norm = np.linalg.norm(x_n, 2)\n",
    "        yp_norm = np.linalg.norm(y_p, 2)\n",
    "        yn_norm = np.linalg.norm(y_n, 2)\n",
    "\n",
    "        p_norm = xp_norm * yp_norm \n",
    "        n_norm = xn_norm * yn_norm \n",
    "        if p_norm > n_norm: \n",
    "            u_i = x_p / xp_norm \n",
    "            v_i = y_p / yp_norm \n",
    "            sigma = p_norm \n",
    "        else: \n",
    "            u_i = x_n / xn_norm \n",
    "            v_i = y_n / yn_norm \n",
    "            sigma = n_norm \n",
    "        w[:, i] = np.sqrt(s[i] * sigma) * u_i\n",
    "        h[i, :] = np.sqrt(s[i] * sigma) * v_i.T \n",
    "\n",
    "    threshold = 1e-10 \n",
    "\n",
    "    w[w < threshold] = 0\n",
    "    h[h < threshold] = 0\n",
    "\n",
    "    return w, h \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "w, h = svd_init(tfidf_df, 3)\n",
    "print(\"w: \", w, w.shape)\n",
    "print(\"h: \", h, h.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['object use', 'pod use', 'pod secur', 'kubernet object']\n",
      "['min read', 'text googl', 'topic model', 'et al']\n",
      "['site new', 'open anoth', 'meet access', 'new window']\n",
      "['open option list', 'open option', 'list convers', 'option list convers']\n",
      "[[1.78744765e-04 1.10787074e-04 5.61728657e-01 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 1.11269572e+00]\n",
      " [1.74038903e-01 1.38208852e-03 0.00000000e+00 2.69484638e-03]\n",
      " [0.00000000e+00 5.38750178e-01 1.08859770e-04 0.00000000e+00]\n",
      " [0.00000000e+00 5.08268889e-01 0.00000000e+00 0.00000000e+00]\n",
      " [5.57761085e-05 6.68618847e-01 1.94308088e-04 4.34908439e-04]\n",
      " [0.00000000e+00 0.00000000e+00 6.59341490e-01 7.86561752e-04]\n",
      " [7.96706614e-01 0.00000000e+00 1.94687868e-04 0.00000000e+00]\n",
      " [7.87029555e-01 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "nmf_model = NMF(n_components=4, random_state=60)\n",
    "\n",
    "\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "W = nmf_model.fit_transform(x)\n",
    "H = nmf_model.components_ \n",
    "\n",
    "\n",
    "topics = [] \n",
    "\n",
    "for index, topic in enumerate(H):\n",
    "    topics.append([terms[i] for i in topic.argsort()[-3:]])\n",
    "    print([terms[i] for i in topic.argsort()[-4:]])\n",
    "\n",
    "\n",
    "print(W)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mapping back to documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "./corpus/spacelift.io_blog_kubernetes-cronjob.txt\n",
      "./corpus/kubernetes.io_docs_concepts_workloads_controllers_cron-jobs_.txt\n",
      "./corpus/kubernetes.io_docs_tasks_job_automated-tasks-with-cron-jobs_.txt\n",
      "\n",
      "Topic 2:\n",
      "./corpus/stackoverflow.com_questions_45019753_how-to-retrain-a-machine-learning-model-in-python-till-we-get-desired-outcome.txt\n",
      "./corpus/towardsdatascience.com_topic-modeling-articles-with-nmf-8c6b2a227a45.txt\n",
      "./corpus/www.frontiersin.org_articles_10.3389_fsoc.2022.886498_full.txt\n",
      "\n",
      "Topic 3:\n",
      "./corpus/www.aa.com_booking_find-flights.txt\n",
      "./corpus/www.jetblue.com_flights.txt\n",
      "\n",
      "Topic 4:\n",
      "./corpus/www.linkedin.com_jobs_search_?currentJobId=3915603480&distance=25&f_WT=1%2C3%2C2&geoId=102448103&keywords=software%20engineer&origin=JOBS_HOME_SEARCH_CARDS.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topic_doc_map = {i: [] for i in range(nmf_model.n_components)}\n",
    "\n",
    "for doc_index, topic_scores in enumerate(W):\n",
    "    max_topic_score = np.argmax(topic_scores)\n",
    "    topic_doc_map[max_topic_score].append(doc_index)\n",
    "\n",
    "for topic, doc_indices in topic_doc_map.items():\n",
    "    print(f\"Topic {topic + 1}:\")\n",
    "    for doc_index in doc_indices:\n",
    "        print(f\"{file_paths[doc_index]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
